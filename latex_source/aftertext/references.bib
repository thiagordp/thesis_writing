%------------------------------------------------------------------ %
%                           INTRODUCTION REFS                       %
%------------------------------------------------------------------ %


%------------------------------------------------------------------ %

@book{CNJ2020,
address = {Bras{\'{i}}lia},
author = {CNJ},
editor = {CNJ},
pages = {236},
publisher = {CNJ},
title = {{Justi{\c{c}}a em N{\'{u}}meros 2020}},
year = {2020}
}

%------------------------------------------------------------------ %

@incollection{Watanabe1985,
  author={Kazuo Watanabe},
  year= 1985, 
  chapter={Filosofia e caracter\'{i}sticas b\'{a}sicas do Juizado Especial de Pequenas Causas}, 
  editor = {Kazuo Watanabe}, 
  booktitle= {Juizado Especial de Pequenas Causas: lei n. 7.244/1984},
  publisher= {Revista dos Tribunais},
  address= {São Paulo}, 
}

%------------------------------------------------------------------ %

@article{Sabo2021,
author = {Sabo, Isabela Cristina and {Dal Pont}, Thiago Raulino and Wilton, Pablo Ernesto Vigneaux and Rover, Aires Jos{\'{e}} and H{\"{u}}bner, Jomi Fred},
doi = {10.1007/s10506-021-09287-3},
file = {:home/trdp/Downloads/Sabo_et_al-2021-Artificial_Intelligence_and_Law.pdf:pdf},
issn = {0924-8463},
journal = {Artificial Intelligence and Law},
keywords = {Air transport service,Consumer law,Unsupervised le,affinity propagation,air transport service,consumer law,hierarchical clustering,k-means,lingo,unsupervised learning},
month = {4},
number = {0123456789},
pages = {1--37},
publisher = {Springer Netherlands},
title = {{Clustering of Brazilian legal judgments about failures in air transport service: an evaluation of different approaches}},
year = {2021}
}

@inproceedings{DalPont2020,
address = {Cham},
author = {{Dal Pont}, Thiago Raulino and Sabo, Isabela Cristina and H{\"{u}}bner, Jomi Fred and Rover, Aires Jos{\'{e}}},
booktitle = {Intelligent Systems},
editor = {Cerri, Ricardo and Prati, Ronaldo C},
isbn = {978-3-030-61377-8},
pages = {521--535},
doi = {10.1007/978-3-030-61377-8_36},
publisher = {Springer International Publishing},
title = {{Impact of Text Specificity and Size on Word Embeddings Performance: An Empirical Evaluation in Brazilian Legal Domain}},
year = {2020}
}


@article{Sabo2019,
author = {Sabo, Isabela Cristina and {Dal Pont}, Thiago Raulino and Rover, Aires Jos{\'{e}} and H{\"{u}}bner, Jomi Fred},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sabo, Raulino, Pont - Unknown - CLASSIFICA{\c{C}}{\~{A}}O DE SENTEN{\c{C}}AS DE JUIZADO ESPECIAL CLASSIFICATION OF SPECIAL CIVIL COURT JUDGMENTS USING.pdf:pdf},
journal = {Revista Democracia Digital e Governo Eletr{\^{o}}nico},
number = {18},
pages = {94--106},
title = {{Classifica{\c{c}}{\~{a}}o de senten{\c{c}}as de Juizado Especial C{\'{i}}vel utilizando aprendizado de m{\'{a}}quina}},
volume = {1},
year = {2019}
}




% ------------------------------------------
%%          INTRODUCTION
% ------------------------------------------
@book{Russell2020,
author = {Russell, Stuart and Norvig, Peter},
isbn = {978-0-13-604259-4},
publisher = {Pearson},
title = {{Artificial Intelligence: A Modern Approach}},
year = {2020}
}

@book{Aggarwal2013,
address = {Boston, MA},
author = {Aggarwal, Charu C. and Zhai, Cheng Xiang},
booktitle = {Mining Text Data},
doi = {10.1007/978-1-4614-3223-4},
editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aggarwal, Zhai - 2012 - Mining Text Data.pdf:pdf},
isbn = {978-1-4614-3222-7},
pages = {1--522},
publisher = {Springer US},
title = {{Mining Text Data}},
volume = {9781461432},
year = {2012}
}

@book{Mitchell1997,
author = {Mitchell, Thomas M},
edition = {1},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell - 1997 - Machine Learning.pdf:pdf},
isbn = {9780070428072},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning},
publisher = {McGraw-Hill Education},
title = {{Machine Learning}},
year = {1997}
}

%% Aplication of machine learning
% Text classifier
@article{Sebastiani2002,
author = {Sebastiani, Fabrizio},
doi = {10.1145/505282.505283},
eprint = {0110053},
issn = {0360-0300},
journal = {ACM Computing Surveys},
keywords = {Machine learning,Text categorization,Text classification},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Context},
month = {3},
number = {1},
pages = {1--47},
primaryClass = {cs},
title = {{Machine learning in automated text categorization}},
volume = {34},
year = {2002}
}

% Detect faces

% Control robots
@article{Kober2013,
abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research. {\textcopyright} The Author(s) 2013.},
author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
doi = {10.1177/0278364913495721},
file = {:home/trdp/Downloads/0278364913495721.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Reinforcement learning,learning control,robot,survey},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Context},
number = {11},
pages = {1238--1274},
title = {{Reinforcement learning in robotics: A survey}},
volume = {32},
year = {2013}
}

% Predict weather
@article{Shi2015,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
archivePrefix = {arXiv},
arxivId = {1506.04214},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
eprint = {1506.04214},
file = {:home/trdp/Downloads/1506.04214.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Context},
month = {6},
pages = {802--810},
title = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
volume = {2015-Janua},
year = {2015}
}


@article{Devlin2018,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% OTher 


@article{Sadiku2020,
  doi = {10.32936/pssj.v4i1.142},
  
  year = {2020},
  month = 4,
  publisher = {Prizren Social Science Journal},
  volume = {4},
  number = {1},
  pages = {50--56},
  author = {Asmir Sadiku},
  title = {Immaterial Damage and Some Types of its Compensation},
  journal = {Prizren Social Science Journal}
}

@article{Brown2020,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2005.14165},
  year      = {2020},
  archivePrefix = {arXiv},
  eprint    = {2005.14165},
  timestamp = {Wed, 03 Jun 2020 11:36:54 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Peters2018,
address = {Stroudsburg, PA, USA},
author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
doi = {10.18653/v1/N18-1202},
pages = {2227--2237},
pmid = {299497},
publisher = {Association for Computational Linguistics},
title = {{Deep Contextualized Word Representations}},
year = {2018}
}


@article{Bibal2020,
  doi = {10.1007/s10506-020-09270-4},
  year = {2020},
  month = 7,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {29},
  number = {2},
  pages = {149--169},
  author = {Adrien Bibal and Michael Lognoul and Alexandre de Streel and Beno{\^{\i}}t Fr{\'{e}}nay},
  title = {Legal requirements on explainability in machine learning},
  journal = {Artificial Intelligence and Law}
}

@article{Tjoa2019,
  author    = {Erico Tjoa and
               Cuntai Guan},
  title     = {A Survey on Explainable Artificial Intelligence {(XAI):} Towards Medical
               {XAI}},
  journal   = {CoRR},
  volume    = {abs/1907.07374},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1907.07374},
  timestamp = {Tue, 23 Jul 2019 10:54:22 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2015 - Deep Learning in neural networks An overview.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning/Deep Learning,Msc Thesis/Thesis Writing Works/Introduction/Context},
month = {1},
pages = {85--117},
pmid = {25462637},
publisher = {Elsevier Ltd},
title = {{Deep learning in neural networks: An overview}},
volume = {61},
year = {2015}
}


@book{Theodoridis2009,
address = {Burlington},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
edition = {4},
publisher = {Academic Press},
title = {{Pattern Recognition}},
year = {2009}
}

@article{Sheikhalishahi2019,
  year = {2019},
  month = apr,
  publisher = {{JMIR} Publications Inc.},
  volume = {7},
  number = {2},
  pages = {e12239},
  author = {Seyedmostafa Sheikhalishahi and Riccardo Miotto and Joel T Dudley and Alberto Lavelli and Fabio Rinaldi and Venet Osmani},
  title = {Natural Language Processing of Clinical Notes on Chronic Diseases: Systematic Review},
  journal = {{JMIR} Medical Informatics}
}

@article{Caruana1997,
author = {Caruana, Rich},
doi = {10.1023/A:1007379606734},
issn = {08856125},
journal = {Machine Learning},
keywords = {Backpropagation,Generalization,Inductive transfer,K-nearest neighbor,Kernel regression,Multitask learning,Parallel transfer,Supervised learning},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Context},
number = {1},
pages = {41--75},
title = {{Multitask Learning}},
volume = {28},
year = {1997}
}

@inproceedings{Undavia2018,
abstract = {In recent years, deep learning has shown promising results when used in
the field of natural language processing (NLP). Neural networks (NNs)
such as convolutional neural networks (CNNs) and recurrent neural
networks (RNNs) have been used for various NLP tasks including sentiment
analysis, information retrieval, and document classification. In this
paper, we the present the Supreme Court Classifier (SCC), a system that
applies these methods to the problem of document classification of legal
court opinions. We compare methods using traditional machine learning
with recent NN-based methods. We also present a CNN used with
pre-trained word vectors which shows improvements over the
state-of-the-art applied to our dataset. We train and evaluate our
system using the Washington University School of Law Supreme Court
Database (SCDB). Our best system (word2vec + CNN) achieves 72.4\%
accuracy when classifying the court decisions into 15 broad SCDB
categories and 31.9\% accuracy when classifying among 279 liner-grained
SCDB categories.},
address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
annote = {Federated Conference on Computer Science and Information Systems
(FedCSIS), Poznan, POLAND, SEP 09-12, 2018},
author = {Undavia, Samir and Meyers, Adam and Ortega, John E},
booktitle = {PROCEEDINGS OF THE 2018 FEDERATED CONFERENCE ON COMPUTER SCIENCE AND INFORMATION SYSTEMS (FEDCSIS)},
doi = {10.15439/2018F227},
editor = {{Ganzha, M and Maciaszek, L and Paprzycki, M}},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Undavia, Meyers, Ortega - 2018 - A Comparative Study of Classifying Legal Documents with Neural Networks(2).pdf:pdf},
isbn = {978-8-3949-4195-6},
issn = {2325-0348},
organization = {Inst Elect \& Elect Engineers; Polish Informat Proc Soc; Polish Informat Proc Soc, Mazovia Chapter; Inst Elect \& Elect Engineers Poland Sect Comp Soc Chapter; Syst Res Inst Polish Acad Sci; Warsaw Univ Technol; Wroclaw Univ Econ; Adam Mickiewicz Univ; In},
pages = {515--522},
publisher = {IEEE},
series = {Federated Conference on Computer Science and Information Systems},
title = {{A Comparative Study of Classifying Legal Documents with Neural Networks}},
type = {Proceedings Paper},
year = {2018}
}


@inproceedings{Sulea2017,
abstract = {In this paper, we investigate the application of text classification methods to predict the law area and the decision of cases judged by the French Supreme Court. We also investigate the influence of the time period in which a ruling was made over the textual form of the case description and the extent to which it is necessary to mask the judge's motivation for a ruling to emulate a real-world test scenario. We report results of 96% f1 score in predicting a case ruling, 90% f1 score in predicting the law area of a case, and 75.9% f1 score in estimating the time span when a ruling has been issued using a linear Support Vector Machine (SVM) classifier trained on lexical features.},
archivePrefix = {arXiv},
arxivId = {1708.01681},
author = {Şulea, Octavia Maria and Zampieri, Marcos and Vela, Mihaela and {Van Genabith}, Josef},
booktitle = {International Conference Recent Advances in Natural Language Processing, RANLP},
doi = {10.26615/978-954-452-049-6-092},
eprint = {1708.01681},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Şulea et al. - 2017 - Predicting the law area and decisions of French supreme court cases.pdf:pdf},
isbn = {9789544520489},
issn = {13138502},
pages = {716--722},
title = {{Predicting the law area and decisions of French supreme court cases}},
volume = {2017-Septe},
year = {2017}
}

@inproceedings{Virtucio2018,
abstract = {For the past decades, Philippine courts have been experiencing severe court congestion and case backlog problems. This study aims to provide a solution to alleviate these problems by predicting the outcome of court cases. As the Philippine Supreme Court case decisions are the only readily available data online, we use this as our dataset. We use Natural Language Processing, particularly the bag-of-words model to represent the case text into n-grams. Spectral clustering is also used to group these n-grams into topics. These n-gram and topic features are then input to the machine learning algorithms such as linear support vector machines and random forest classifiers. Linear support vector machine results reached 45% on the n-gram datasets and 55% on the topic datasets. The best result we obtained is 59% on the topic datasets using a random forest classifier. This is the first systematic study in predicting Philippine Supreme Court decisions based purely on textual content.},
author = {Virtucio, Michael Benedict L. and Aborot, Jeffrey A. and Abonita, John Kevin C. and Avinante, Roxanne S. and Copino, Rother Jay B. and Neverida, Michelle P. and Osiana, Vanesa O. and Peramo, Elmer C. and Syjuco, Joanna G. and Tan, Glenn Brian A.},
booktitle = {2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},
doi = {10.1109/COMPSAC.2018.10348},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Virtucio et al. - 2018 - Predicting Decisions of the Philippine Supreme Court Using Natural Language Processing and Machine Learning.pdf:pdf},
isbn = {978-1-5386-2666-5},
issn = {07303157},
keywords = {Artificial-intelligence,Judiciary,Law,Machine-learning,NLP,Philippines},
month = {7},
pages = {130--135},
publisher = {IEEE},
title = {{Predicting Decisions of the Philippine Supreme Court Using Natural Language Processing and Machine Learning}},
volume = {2},
year = {2018}
}

@article{Hassan2020,
abstract = {Contract documents are a critical legal component of a construction project that specify all wishes and expectations of the owner toward the design, construction, and handover of a project. Precise comprehension of the contract documents is critical to ensure that all important contractual requirements of the project scope are captured and managed. A contract package typically includes both requirements and other unimportant texts such as instructions and supporting statements; thus, practitioners are required to read and identify texts indicating the requirements. The conventional manual practice of scope comprehension requires much time and effort and may include human errors. Little attention has been paid toward automated identification of requirement texts. This study introduces an effective way to identify contractual requirements by developing an automated framework using natural language processing (NLP) and machine learning techniques. Four different machine learning algorithms, namely Na{\"{i}}ve Bayes, support vector machines, logistic regression, and feedforward neural network were used to develop the classification models. The models classified the contractual text into requirement and nonrequirement text. Experiments showed that the support vector machine model outperforms the other models in terms of accuracy, precision, recall, and F1-score. In addition, unigrams yield better results than higher n-gram features. An experimental study including human participants further proves that the developed model is efficient and effective that can help reduce reading time and improve contract scope comprehension.},
author = {Hassan, Fahad Ul and Le, Tuyen},
doi = {10.1061/(ASCE)LA.1943-4170.0000379},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassan, Le - 2020 - Automated Requirements Identification from Construction Contract Documents Using Natural Language Processing.pdf:pdf},
issn = {1943-4162},
journal = {Journal of Legal Affairs and Dispute Resolution in Engineering and Construction},
keywords = {Construction contracts,Machine learning,Natural language processing,Project management,Project requirements,Scope comprehension,Text classification},
month = {5},
number = {2},
pages = {04520009},
title = {{Automated Requirements Identification from Construction Contract Documents Using Natural Language Processing}},

volume = {12},
year = {2020}
}


@article{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = {5},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
volume = {521},
year = {2015}
}

@article{Khan2014,
abstract = {Big Data has gained much attention from the academia and the IT industry. In the digital and computing world, information is generated and collected at a rate that rapidly exceeds the boundary range. Currently, over 2 billion people worldwide are connected to the Internet, and over 5 billion individuals own mobile phones. By 2020, 50 billion devices are expected to be connected to the Internet. At this point, predicted data production will be 44 times greater than that in 2009. As information is transferred and shared at light speed on optic fiber and wireless networks, the volume of data and the speed of market growth increase. However, the fast growth rate of such large data generates numerous challenges, such as the rapid growth of data, transfer speed, diverse data, and security. Nonetheless, Big Data is still in its infancy stage, and the domain has not been reviewed in general. Hence, this study comprehensively surveys and classifies the various attributes of Big Data, including its nature, definitions, rapid growth rate, volume, management, analysis, and security. This study also proposes a data life cycle that uses the technologies and terminologies of Big Data. Future research directions in this field are determined based on opportunities and several open issues in Big Data domination. These research directions facilitate the exploration of the domain and the development of optimal techniques to address Big Data.},
author = {Khan, Nawsher and Yaqoob, Ibrar and Hashem, Ibrahim Abaker Targio and Inayat, Zakira and {Mahmoud Ali}, Waleed Kamaleldin and Alam, Muhammad and Shiraz, Muhammad and Gani, Abdullah},
doi = {10.1155/2014/712826},
file = {:home/trdp/Downloads/712826.pdf:pdf},
issn = {1537744X},
journal = {Scientific World Journal},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Context},
number = {January 2018},
pmid = {25136682},
title = {{Big data: Survey, technologies, opportunities, and challenges}},
volume = {2014},
year = {2014}
}


@article{Trusov2016,
abstract = {Different approaches for textual feature extraction have been proposed starting with simple word count features and continuing with deeper representations capturing distributional semantics. In recent publications word embedding methods have been successfully used as a representation basis for a large number of NLP tasks like text classification, part of speech tagging and many others. In this article we explore opportunities of using multiple text representations simultaneously within one regression task in order to exploit conventional bag of words approach with the more semantically rich embeddings. We investigate performance of this multi-representation approach on the financial risk prediction problem. Publicly available 10-K reports filled by US trading companies are used as the basis for predicting next year change in stock price volatility. Our study shows that models based on single representations achieve performance that is comparable to the previously published results on risk prediction and models with multiple representations benefit from complementary information and outperform both baseline and single representation models.},
author = {Trusov, Roman and Natekin, Alexey and Kalaidin, Pavel and Ovcharenko, Sergey and Knoll, Alois and Fazylova, Aida},
doi = {10.1109/AINL-ISMW-FRUCT.2015.7382979},
isbn = {9789526839707},
journal = {Proceedings of Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference, AINL-ISMW FRUCT 2015},
mendeley-groups = {Msc Thesis/Regression Project/Related Work},
pages = {110--117},
title = {{Multi-representation approach to text regression of financial risks}},
volume = {7},
year = {2016}
}

@article{Medvedeva2019,
abstract = {When courts started publishing judgements, big data analysis (i.e. large-scale statistical analysis of case law and machine learning) within the legal domain became possible. By taking data from the European Court of Human Rights as an example, we investigate how natural language processing tools can be used to analyse texts of the court proceedings in order to automatically predict (future) judicial decisions. With an average accuracy of 75\% in predicting the violation of 9 articles of the European Convention on Human Rights our (relatively simple) approach highlights the potential of machine learning approaches in the legal domain. We show, however, that predicting decisions for future cases based on the cases from the past negatively impacts performance (average accuracy range from 58 to 68\%). Furthermore, we demonstrate that we can achieve a relatively high classification performance (average accuracy of 65\%) when predicting outcomes based only on the surnames of the judges that try the case.},
annote = {This paper has similar decisions as ours...},
author = {Medvedeva, Masha and Vols, Michel and Wieling, Martijn},
doi = {10.1007/s10506-019-09255-y},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Medvedeva, Vols, Wieling - 2019 - Using machine learning to predict decisions of the European Court of Human Rights.pdf:pdf},
issn = {15728382},
journal = {Artificial Intelligence and Law},
keywords = {Case law,European Court of Human Rights,Judicial decisions,Machine learning,Natural language processing},
title = {{Using machine learning to predict decisions of the European Court of Human Rights}},
year = {2019}
}


@inproceedings{Zhang2019,
abstract = {In view of such problems as the difficulty in updating the clustering results of text sets in real time after clustering, and the difficulty in retrieving the regulations of specific content caused by the continuous promulgation of laws and regulations, this paper introduces the Doc2Vec deep learning algorithm to improve the ability of text retrieval and updating the text classification results. Text similarity calculation has great research value in the field of statistical machine translation. Doc2Vec deep learning algorithm can update the clustering result by comparing text similarity without re-implementing text clustering. This paper will use the civil aviation legal provisions as data samples, use Doc2Vec deep learning algorithm to calculate the similarity between different legal provisions, and apply the method to the classification of legal provisions and the update of text clustering results. The experimental results show that although the method does not achieve the ideal expectation of the similarity between the texts in the same field, it can still classify the text more accurately.},
author = {Zhang, Haoyang and Zhou, Liang},
booktitle = {2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
doi = {10.1109/CISP-BMEI48845.2019.8965709},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - 2019 - Similarity Judgment of Civil Aviation Regulations Based on Doc2Vec Deep Learning Algorithm.pdf:pdf},
isbn = {978-1-7281-4852-6},
keywords = {Doc2Vec,text clustering,text similarity,text vectorization},
month = {10},
pages = {1--8},
publisher = {IEEE},
title = {{Similarity Judgment of Civil Aviation Regulations Based on Doc2Vec Deep Learning Algorithm}},
year = {2019}
}



@article{DalPont2021,
author = {{Dal Pont}, Thiago Raulino and Sabo, Isabela Cristina and H, Jomi Fred},
keywords = {consumer law,immaterial damage compensation,natural language processing,text regression},
title = {{Regression in Brazilian Legal Judgments to Predict Compensation for Immaterial Damage}},
journal = {Natural Language Engineering},
year = {2021}
}



@article{Braz2018,
abstract = {The Brazilian court system is currently the most clogged up judiciary system in the world. Thousands of lawsuit cases reach the supreme court every day. These cases need to be analyzed in order to be associated to relevant tags and allocated to the right team. Most of the cases reach the court as raster scanned documents with widely variable levels of quality. One of the first steps for the analysis is to classify these documents. In this paper we present a Bidirectional Long Short-Term Memory network (Bi-LSTM) to classify these pieces of legal document.},
archivePrefix = {arXiv},
arxivId = {1811.11569},
author = {Braz, Fabricio Ataides and da Silva, Nilton Correia and de Campos, Teofilo Emidio and Chaves, Felipe Borges S. and Ferreira, Marcelo H. S. and Inazawa, Pedro Henrique and Coelho, Victor H. D. and Sukiennik, Bernardo Pablo and de Almeida, Ana Paula Goncalves Soares and Vidal, Flavio Barros and Bezerra, Davi Alves and Gusmao, Davi B. and Ziegler, Gabriel G. and Fernandes, Ricardo V. C. and Zumblick, Roberta and Peixoto, Fabiano Hartmann},
eprint = {1811.11569},
title = {{Document classification using a Bi-LSTM to unclog Brazil's Supreme Court}},
year = {2018}
}

@book{Draper1998,
address = {New York},
author = {Draper, Norman Richard and Smith, Harry},
edition = {3},
isbn = {0471221708},
keywords = {Mathematische{\_}Statistik Methode Statistik},
publisher = {Wiley},
series = {Wiley series in probability and mathematical statistics},
title = {{Applied regression analysis}},
year = {1998}
}
@inproceedings{SmywiskiPohl2019,
  year = {2019},
  publisher = {{ACM} Press},
  author = {Aleksander Smywi{\'{n}}ski-Pohl and Karol Lasocki and Krzysztof Wr{\'{o}}bel and Marek Strza{\l}ta},
  title = {Automatic Construction of a Polish Legal Dictionary with Mappings to Extra-Legal Terms Established via Word Embeddings},
  booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law  - {ICAIL} {\textquotesingle}19}
}


@article{Chalkidis2019,
  doi = {10.1007/s10506-018-9238-9},
  year = {2018},
  month = 12,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {27},
  number = {2},
  pages = {171--198},
  author = {Ilias Chalkidis and Dimitrios Kampas},
  title = {Deep learning in law: early adaptation and legal word embeddings trained on large corpora},
  journal = {Artificial Intelligence and Law}
}

@article{Davis2020,
abstract = {Abstract This article explores the future for lawyers and law firms in the light of the changes that Artificial Intelligence (“AI”) is already bringing to the universe of legal services. Part I briefly describes some of the ways AI is already in use in ordinary life - from facial recognition, through medical diagnosis to translation services. Part II describes how AI is transforming what it means to provide legal services in six primary areas: litigation review; expertise automation; legal research; contract analytics; contract and litigation document generation; and predictive analytics. Part III explores who are the providers of these AI driven legal services - often non-lawyer legal service providers - and how these providers are replacing at least some of what clients have traditionally sought from lawyers. Part III also discusses the implications of all these changes both for the future role of lawyers individually, and in particular what services will clients still need lawyers to perform: judgment, empathy, creativity and adaptability. In turn, this Part examines what will these changes mean for the size, shape, composition and economic model of law firms, as well as the implications of these changes for legal education and lawyer training. Part IV identifies the principal legal, ethical, regulatory and risk management issues raised by the use of AI in the provision of legal services. Finally, in Part V the article considers who will be the likely providers of AI based services other than law firms: legal publishers, major accounting firms and venture capital funded businesses.},
author = {Davis, Anthony E.},
doi = {10.1590/2317-6172201945},
file = {:home/trdp/Downloads/81684-Texto do Artigo-174327-1-10-20200608.pdf:pdf},
issn = {2317-6172},
journal = {Revista Direito GV},
keywords = {Artificial Intelligence (AI),Future of Legal Services,Law Firms,Lawyers,Legal Service Providers},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Justification},
number = {1},
pages = {1DUMMT},
title = {{The Future of Law Firms (and Lawyers) in the Age of Artificial Intelligence}},
volume = {16},
year = {2020}
}


@article{Cath2018,
abstract = {This paper is the introduction to the special issue entitled: ‘Governing artificial intelligence: ethical, legal and technical opportunities and challenges'. Artificial intelligence (AI) increasingly permeates every aspect of our society, from the critical, like urban infrastructure, law enforcement, banking, healthcare and humanitarian aid, to the mundane like dating. AI, including embodied AI in robotics and techniques like machine learning, can improve economic, social welfare and the exercise of human rights. Owing to the proliferation of AI in high-risk areas, the pressure is mounting to design and govern AI to be accountable, fair and transparent. How can this be achieved and through which frameworks? This is one of the central questions addressed in this special issue, in which eight authors present in-depth analyses of the ethical, legal-regulatory and technical challenges posed by developing governance regimes for AI systems. It also gives a brief overview of recent developments in AI governance, how much of the agenda for defining AI regulation, ethical frameworks and technical approaches is set, as well as providing some concrete suggestions to further the debate on AI governance.},
author = {Cath, Corinne},
doi = {10.1098/rsta.2018.0080},
file = {:home/trdp/Downloads/rsta.2018.0080.pdf:pdf},
issn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Artificial intelligence,Culture,Ethics,Governance,Law,Technology},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Justification},
month = {11},
number = {2133},
pages = {20180080},
pmid = {30322996},
title = {{Governing artificial intelligence: ethical, legal and technical opportunities and challenges}},
volume = {376},
year = {2018}
}


@misc{CNJ2020b,
address = {Bras{\'{i}}lia},
author = {CNJ},
title = {{Portaria N 271}},
url = {https://atos.cnj.jus.br/atos/detalhar/3613},
year = {2020}
}

@misc{DictJudgment2021,
author = {Gerald Hill and Kathleen Hill},
title = {Legal Dictionary - Judgment},
url = {https://dictionary.law.com/Default.aspx?selected=1056},
year = {2021}
}

@inproceedings{Hammami2019,
abstract = {In current years, deep learning has showed promising results when used in the field of natural language processing (NLP). Neural Networks (NNs) such as convolutional neural network (CNN) and recurrent neural network (RNN) have been utilized for different NLP tasks like information retrieval, sentiment analysis and document classification. In this paper, we explore the use of NNs-based method for legal text classification. In our case, the results show that NN models with a fixed input length outperforms baseline methods.},
author = {Hammami, Eya and Akermi, Imen and Faiz, Rim and Boughanem, Mohand},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-32065-2_7},
isbn = {9783030320645},
issn = {16113349},
keywords = {Convolutional Neural Networks,Deep learning,Document categorization,Legal domain,Natural Language Processing},
mendeley-groups = {Msc Thesis/RSL - Text Mining & Law/Selected Papers},
pages = {96--105},
title = {{Deep Learning for French Legal Data Categorization}},
volume = {11815 LNCS},
year = {2019}
}

@article{Hassan2020,
abstract = {Contract documents are a critical legal component of a construction project that specify all wishes and expectations of the owner toward the design, construction, and handover of a project. Precise comprehension of the contract documents is critical to ensure that all important contractual requirements of the project scope are captured and managed. A contract package typically includes both requirements and other unimportant texts such as instructions and supporting statements; thus, practitioners are required to read and identify texts indicating the requirements. The conventional manual practice of scope comprehension requires much time and effort and may include human errors. Little attention has been paid toward automated identification of requirement texts. This study introduces an effective way to identify contractual requirements by developing an automated framework using natural language processing (NLP) and machine learning techniques. Four different machine learning algorithms, namely Na{\"{i}}ve Bayes, support vector machines, logistic regression, and feedforward neural network were used to develop the classification models. The models classified the contractual text into requirement and nonrequirement text. Experiments showed that the support vector machine model outperforms the other models in terms of accuracy, precision, recall, and F1-score. In addition, unigrams yield better results than higher n-gram features. An experimental study including human participants further proves that the developed model is efficient and effective that can help reduce reading time and improve contract scope comprehension.},
author = {Hassan, Fahad Ul and Le, Tuyen},
doi = {10.1061/(ASCE)LA.1943-4170.0000379},
issn = {1943-4162},
journal = {Journal of Legal Affairs and Dispute Resolution in Engineering and Construction},
keywords = {Construction contracts,Machine learning,Natural language processing,Project management,Project requirements,Scope comprehension,Text classification},
month = {may},
number = {2},
pages = {04520009},
title = {{Automated Requirements Identification from Construction Contract Documents Using Natural Language Processing}},
volume = {12},
year = {2020}
}


@book{Cury2019,
author = {Cury, Augusto and Watanabe, Kazuo and Salom{\~{a}}o, Luis Felipe and Lamachia, Claudio and Lagrasta, Valeria Ferioli and Navarro, Henrique {\'{A}}vila e Tr{\'{i}}cia and Cabral, Xavier and de Souza, Luiz Pontel and da Fonseca, Reynaldo Soares da Fonseca e Gabriel Campos Soares and Neto, Jayme Martins de Oliveira and Fernandes, Grace Maria and de Azevedo, J{\'{u}}lio Camargo and da Costa, Domingos Barroso and Rudolfo, Fernanda Mambrini and Leite, Antonio Jos{\'{e}} Maffezoli},
edition = {1},
isbn = {978-8530982089},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Justification},
pages = {250},
publisher = {Forense},
title = {{Solu{\c{c}}{\~{o}}es Pac{\'{i}}ficas de Conflitos: Para um Brasil Moderno}},
year = {2019}
}

@book{Mancuso2020,
address = {Belo Horizonte},
author = {Mancuso, Rodolfo de Camargo},
edition = {3},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Introduction/Justification},
pages = {912},
publisher = {Juspodivm},
title = {{A Resolu{\c{c}}{\~{a}}o dos Conflitos e a Fun{\c{c}}{\~{a}}o Judicial no Contempor{\^{a}}neo Estado de Direito}},
year = {2020}
}


@incollection{Rodrigues2020,
  year = {2020},
  publisher = {Springer International Publishing},
  pages = {239--248},
  author = {Ruan Chaves Rodrigues and J{\'{e}}ssica Rodrigues and Pedro Vitor Quinta de Castro and N{\'{a}}dia Felix Felipe da Silva and Anderson Soares},
  title = {Portuguese Language Models and Word Embeddings: Evaluating on Semantic Similarity Tasks},
  booktitle = {Lect Notes Comput Sc}
}


@inproceedings{joshi2010,
    title = "Movie Reviews and Revenues: An Experiment in Text Regression",
    author = "Joshi, Mahesh  and
      Das, Dipanjan  and
      Gimpel, Kevin  and
      Smith, Noah A.",
    booktitle = "Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = 6,
    year = "2010",
    address = "Los Angeles, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N10-1038",
    pages = "293--296",
}


@inproceedings{lampos2014,
    title = "Predicting and Characterising User Impact on {T}witter",
    author = "Lampos, Vasileios  and
      Aletras, Nikolaos  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Cohn, Trevor",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    doi = "10.3115/v1/E14-1043",
    pages = "405--413"
}

@inproceedings{Zou2016,
author = {Zou, Bin and Lampos, Vasileios and Gorton, Russell and Cox, Ingemar J.},
title = {On Infectious Intestinal Disease Surveillance Using Social Media Content},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2896338.2896372},
abstract = {This paper investigates whether infectious intestinal diseases (IIDs) can be detected and quantified using social media content. Experiments are conducted on user-generated data from the microblogging service, Twitter. Evaluation is based on the comparison with the number of IID cases reported by traditional health surveillance methods. We employ a deep learning approach for creating a topical vocabulary, and then apply a regularised linear (Elastic Net) as well as a nonlinear (Gaussian Process) regression function for inference. We show that like previous text regression tasks, the nonlinear approach performs better. In general, our experimental results, both in terms of predictive performance and semantic interpretation, indicate that Twitter data contain a signal that could be strong enough to complement conventional methods for IID surveillance.},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
pages = {157–161},
numpages = {5},
keywords = {disease surveillance, iid, twitter, word embeddings, social media, user-generated content, infectious intestinal disease},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}


@inproceedings{Kusmierczyk2016,
author = {Kusmierczyk, Tomasz and N\o{}rv\r{a}g, Kjetil},
title = {Online Food Recipe Title Semantics: Combining Nutrient Facts and Topics},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2983323.2983897},
abstract = {Dietary pattern analysis is an important research area, and recently the availability of rich resources in food-focused social networks has enabled new opportunities in that field. However, there is a little understanding of how online textual content is related to actual health factors, e.g., nutritional values. To contribute to this lack of knowledge, we present a novel approach to mine and model online food content by combining text topics with related nutrient facts. Our empirical analysis reveals a strong correlation between them and our experiments show the extent to which it is possible to predict nutrient facts from meal name.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {2013–2016},
numpages = {4},
keywords = {online food recipe, text regression, LDA, social media mining},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@article{XU2020,
title = "Utilizing the platform economy effect through EWOM: Does the platform matter?",
journal = "International Journal of Production Economics",
volume = "227",
pages = "107663",
year = "2020",
issn = "0925-5273",
doi = "10.1016/j.ijpe.2020.107663",
author = "Xun Xu and Chieh Lee",
keywords = "Platform economy, Platform, Consumer satisfaction, Online consumer reviews, Online managerial response",
abstract = "Businesses use the platform economy and electronic word-of-mouth generated by online reviews to attract consumers. Based on the communication accommodation theory, we examine both the contents of online consumer reviews and managerial responses to analyze what consumers and managers write and the associated linguistic characteristics to determine how they write. We examine and compare online consumer reviews and managerial responses on social media platforms, third-party booking platforms, and direct-sales platforms in four dimensions. These include (a) linguistic characteristics, (b) content, (c) the interaction between consumers and managers, and (d) the mechanism of the reflection of online reviews in consumer satisfaction. We use a text mining approach, latent semantic analysis, and text regressions to analyze data from the hotel industry. The findings suggest that both the linguistic characteristics and content of the consumer reviews and managerial responses differ depending on the platform. However, the factors influencing consumer satisfaction are the same among the three platforms. Consumer reviews on social media platforms have greater subjectivity and length; reviews on direct platforms have higher polarity, diversity, and readability. Consumer reviews focus more on interpersonal and intangible attributes on social media, on economic attributes on third-party platforms, and on tangible attributes on direct platforms. Managers’ responses have similar linguistic styles in terms of subjectivity, polarity, and readability, but they use more words with greater diversity and focus more on operations and facility issues on direct platforms than on the other two platforms. We provide implications for managers to understand consumer reviews and write responses."
}

%=====================================================%
%              EXPERIMENTS RESULTS                    %
%=====================================================%


@manual{Jusbrasil2020,
  title  = "JusBrasil. Conectando pessoas \`{a} justiça",
  author = "JusBrasil",
  url    = "https://www.jusbrasil.com.br/home",
  year   = "2020",
  note   = {{A}ccessed in {F}eb 10, 2020}
}

@manual{STF2020,
  title  = "Supremo Tribunal Federal",
  author = "STF",
  url    = "http://portal.stf.jus.br/",
  year   = "2020",
  note   = {{A}ccessed in {F}eb 15, 2020}
}

@manual{STJ2020,
  title  = "STJ - Jurisprud\^{e}ncia do STJ",
  author = "STJ",
  url    = "https://scon.stj.jus.br/SCON/",
  year   = "2020",
  note   = {{A}ccessed in {F}eb 05, 2020}
} 
@manual{TJSC2020,
  title  = "Jurisprud\^{e}ncia Catarinense - TJSC",
  author = "TJSC",
  url    = "http://busca.tjsc.jus.br/jurisprudencia/",
  year   = "2020",
  note   = {{A}ccessed in {F}eb 09, 2020}
} 

@misc{Wikipedia2019, 
    title={{PT} {Wiki} dump progress on 20191120}, 
    author={Wikipedia},
    url={http://wikipedia.c3sl.ufpr.br/ptwiki/20191120/}, 
    publisher={Wikipedia},
    year = {2019}
} 

@misc{Tatman2017, 
    title={Brazilian Literature Books}, 
    url={https://www.kaggle.com/rtatman/brazilian-portuguese-literature-corpus}, 
    author = {Rachael Tatman},
    publisher={Kaggle},
    year = {2017}
} 

@misc{Marlessonn2019, 
    title={News of the Brazilian Newspaper}, 
    url={https://www.kaggle.com/marlesson/news-of-the-site-folhauol}, 
    author = {Marlessonn},
    publisher={Kaggle},
    year = {2019}
} 

@misc{Tan2020, 
    title={Old Newspapers}, 
    url={https://www.kaggle.com/alvations/old-newspapers}, 
    author = {Liling Tan},
    publisher={Kaggle},
    year = {2020}
} 

@misc{Christensen2016, 
    title={{HC} {C}orpora}, 
    url={https://web.archive.org/web/20161021044006/http://corpora.heliohost.org/}, 
    author = {Hans Christensen},
    publisher={Kaggle},
    year = {2016}
} 

@inproceedings{Santos2018,
    address = {Miyazaki, Japan},
    author = {Santos, Henrique and Woloszyn, Vinicius and Vieira, Renata},
    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
    publisher = {European Language Resources Association (ELRA)},
    title = {{B}log{S}et-{BR}: A {B}razilian {P}ortuguese {B}log {C}orpus},
    year = {2018}
}


@inproceedings{Chocron2018,
doi = {10.24963/ijcai.2018/22},
  title     = {Vocabulary Alignment for Collaborative Agents: a Study with Real-World Multilingual How-to Instructions},
  author    = {Paula Chocron and Paolo Pareti},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {159--165},
  year      = {2018},
  month     = {7},
}

@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781},
file = {:home/trdp/Downloads/1301.3781.pdf:pdf},
journal = {1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings},
mendeley-groups = {Msc Thesis/Embeddings Project},
month = {1},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}

@inproceedings{Pennington2014,
abstract = {Manufacturing musicassettes requires special measures to obtain a degree of interchangeability acceptable to the ultimate consumer. Dimensional and temperature stability of semifinished parts is a major contributing factor which, together with an intensive control on duplicating and assembling processes, can guarantee a sound product.},
address = {Stroudsburg, PA, USA},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
file = {:home/trdp/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/HANSON ER - 1971 - GLOVE.pdf:pdf},
issn = {00047554},
number = {5},
pages = {1532--1543},
publisher = {Association for Computational Linguistics},
title = {{Glove: Global Vectors for Word Representation}},
volume = {19},
year = {2014}
}


@inproceedings{Kim2014,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This paper show a series of experiments with Convolutional Neural Networks for sentence-level classification tasks with different hyperparameter settings and how sensitive model performance is to changes in these configurations.},
address = {Stroudsburg, PA, USA},
author = {Kim, Yoon},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
isbn = {978-1-5386-3057-0},
keywords = {Deep neural network,Natural language processing,Sentiment analysis},
month = {9},
pages = {1746--1751},
publisher = {Association for Computational Linguistics},
title = {{Convolutional Neural Networks for Sentence Classification}},
volume = {2017-Janua},
year = {2014}
}



@book{Cohen1995,
author = {Cohen, Paul R.},
title = {Empirical Methods for Artificial Intelligence},
year = {1995},
isbn = {0262032252},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}

 @article{Demsar13,
  author  = {Janez Dem{\v{s}}ar and Toma{\v{z}} Curk and Ale{\v{s}} Erjavec and {\v{C}}rt Gorup and Toma{\v{z}} Ho{\v{c}}evar and Mitar Milutinovi{\v{c}} and Martin Mo{\v{z}}ina and Matija Polajnar and Marko Toplak and An{\v{z}}e Stari{\v{c}} and Miha {\v{S}}tajdohar and Lan Umek and Lan {\v{Z}}agar and Jure {\v{Z}}bontar and Marinka {\v{Z}}itnik and Bla{\v{z}} Zupan},
  title   = {Orange: Data Mining Toolbox in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2013},
  volume  = {14},
  number  = {35},
  pages   = {2349-2353}
}
    
    
@misc{Chollet2015,
    title={Keras},
    author={Chollet, Fran\c{c}ois and others},
    year={2015},
    howpublished={\url{https://keras.io}},
} 

@INPROCEEDINGS{Loper02,
    author = {Edward Loper and Steven Bird},
    title = {NLTK: The Natural Language Toolkit},
    booktitle = {In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics},
    year = {2002}
}

@InProceedings{ Mckinney2010,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  doi       = { 10.25080/Majora-92bf1922-00a }
}

@Article{Hunter:2007,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}

@article{Pedregosa2012,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Porter1980,
  author = {Porter, Martin F},
  journal = {Program},
  keywords = {porter snowball stemming},
  number = 3,
  pages = {130--137},
  publisher = {MCB UP Ltd},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {An algorithm for suffix stripping},
  volume = 14,
  year = 1980
}



@article{Airola2011,
abstract = {Reliable estimation of the classification performance of inferred predictive models is difficult when working with small data sets. Cross-validation is in this case a typical strategy for estimating the performance. However, many standard approaches to cross-validation suffer from extensive bias or variance when the area under the ROC curve (AUC) is used as the performance measure. This issue is explored through an extensive simulation study. Leave-pair-out cross-validation is proposed for conditional AUC-estimation, as it is almost unbiased, and its deviation variance is as low as that of the best alternative approaches. When using regularized least-squares based learners, efficient algorithms exist for calculating the leave-pair-out cross-validation estimate. {\textcopyright} 2010 Published by Elsevier B.V.},
author = {Airola, Antti and Pahikkala, Tapio and Waegeman, Willem and {De Baets}, Bernard and Salakoski, Tapio},
doi = {10.1016/j.csda.2010.11.018},
file = {:home/trdp/Downloads/1-s2.0-S0167947310004469-main.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
keywords = {Area under the ROC curve,Classifier performance estimation,Conditional AUC estimation,Cross-validation,Leave-pair-out cross-validation},
month = {4},
number = {4},
pages = {1828--1844},
publisher = {Elsevier B.V.},
title = {{An experimental comparison of cross-validation techniques for estimating the area under the ROC curve}},
volume = {55},
year = {2011}
}
@article{Hartmann2017,
abstract = {Word embeddings have been found to provide meaningful representations for words in an efficient way; therefore, they have become common in Natural Language Processing sys- tems. In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants. We trained 31 word embedding models using FastText, GloVe, Wang2Vec and Word2Vec. We evaluated them intrinsically on syntactic and semantic analogies and extrinsically on POS tagging and sentence semantic similarity tasks. The obtained results suggest that word analogies are not appropriate for word embedding evaluation; task-specific evaluations appear to be a better option.},
archivePrefix = {arXiv},
arxivId = {1708.06025},
author = {Hartmann, Nathan and Fonseca, Erick and Shulby, Christopher and Treviso, Marcos and Rodrigues, Jessica and Aluisio, Sandra},
eprint = {1708.06025},
month = {8},
number = {Section 3},
title = {{Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks}},
year = {2017}
}

@inproceedings{Gal2016,
    author = {Gal, Yarin and Ghahramani, Zoubin},
    title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
    year = {2016},
    isbn = {9781510838819},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments
    in deep learning. Yet a major difficulty with these models is their tendency to overfit,
    with dropout shown to fail when applied to recurrent layers. Recent results at the
    intersection of Bayesian modelling and deep learning offer a Bayesian interpretation
    of common deep learning techniques such as dropout. This grounding of dropout in approximate
    Bayesian inference suggests an extension of the theoretical results, offering insights
    into the use of dropout with RNN models. We apply this new variational inference based
    dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment
    analysis tasks. The new approach outperforms existing techniques, and to the best
    of our knowledge improves on the single model state-of-the-art in language modelling
    with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational
    tools in deep learning.},
    booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
    pages = {1027–1035},
    numpages = {9},
    location = {Barcelona, Spain},
    series = {NIPS'16}
}


@article{ElJelali2015a,
abstract = {{\textcopyright} 2015, Springer Science+Business Media Dordrecht. The perspective of online dispute resolution (ODR) is to develop an online electronic system aimed at solving out-of-court disputes. Among ODR schemes, eMediation is becoming an important tool for encouraging the positive settlement of an agreement among litigants. The main motivation underlying the adoption of eMediation is the time/cost reduction for the resolution of disputes compared to the ordinary justice system. In the context of eMediation, a fundamental requirement that an ODR system should meet relates to both litigants and mediators, i.e. to enable an informed negotiation by informing the parties about the rights and duties related to the case. In order to match this requirement, we propose an information retrieval system able to retrieve relevant court decisions with respect to the disputant case description. The proposed system combines machine learning and natural language processing techniques to better match disputant case descriptions (informal and concise) with court decisions (formal and verbose). Experimental results confirm the ability of the proposed solution to empower court decision retrieval, enabling therefore a well-informed eMediation process.},
author = {{El Jelali}, Soufiane and Fersini, Elisabetta and Messina, Enza},
doi = {10.1007/s10506-015-9162-1},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/El Jelali, Fersini, Messina - 2015 - Legal retrieval as support to eMediation matching disputant's case and court decisions.pdf:pdf},
issn = {0924-8463},
journal = {Artificial Intelligence and Law},
keywords = {Information retrieval,Machine learning,Natural language processing,eMediation},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning/Applications/TM & Law,Msc Thesis/Thesis Writing Works/Introduction/Context,Msc Thesis/Thesis Writing Works/Related Work},
month = {3},
number = {1},
pages = {1--22},
title = {{Legal retrieval as support to eMediation: matching disputant's case and court decisions}},
volume = {23},
year = {2015}
}
@article{Aletras2016,
abstract = {Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used to unveil patterns driving judicial decisions. This can be useful, for both lawyers and judges, as an assisting tool to rapidly identify cases and extract patterns which lead to certain decisions. This paper presents the first systematic study on predicting the outcome of cases tried by the European Court of Human Rights based solely on textual content. We formulate a binary classification task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i.e., N-grams, and topics. Our models can predict the court's decisions with a strong accuracy (79\% on average). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision-making is significantly affected by the stimulus of the facts. We also observe that the topical content of a case is another important feature in this classification task and explore this relationship further by conducting a qualitative analysis.},
author = {Aletras, Nikolaos and Tsarapatsanis, Dimitrios and Preoţiuc-Pietro, Daniel and Lampos, Vasileios},
doi = {10.7717/peerj-cs.93},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aletras et al. - 2016 - Predicting judicial decisions of the European Court of Human Rights a Natural Language Processing perspective.pdf:pdf},
issn = {2376-5992},
journal = {PeerJ Computer Science},
keywords = {Artificial intelligence,Judicial decisions,Legal science,Machine learning,Natural language processing,Text mining},
mendeley-groups = {Msc Thesis/RSL - Text Mining & Law/Selected Papers,Msc Thesis/Thesis Writing Works/Related Work},
month = 10,
number = {10},
pages = {e93},
title = {{Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective}},
volume = {2},
year = {2016}
}
@incollection{Silva2018Book,
address = {Belo Horizonte},
author = {Silva, Nilton},
booktitle = {II Congresso Internacional de Direito, Governo e Tecnologia},
chapter = {3},
edition = {1},
editor = {Fernandes, Ricardo Vieira de Carvalho and de Carvalho, Angelo Gamba Prata},
pages = {89--94},
publisher = {F{\'{o}}rum},
title = {{Notas iniciais sobre a evolu{\c{c}}{\~{a}}o dos algoritmos do Victor: o primeiro projeto de intelig{\^{e}}ncia artificial em supremas cortes do mundo}},
year = {2018}
}


@inproceedings{Silva2018,
abstract = {The Brazilian court system is currently the most clogged up judiciary system in the world. Thousands of lawsuit cases reach the supreme court every day. These cases need to be analyzed in order to be associated to relevant tags and allocated to the right team. Most of the cases reach the court as raster scanned documents with widely variable levels of quality. One of the first steps for the analysis is to classify these documents. In this paper we present a Bidirectional Long Short-Term Memory network (Bi-LSTM) to classify these pieces of legal document.},
author = {Silva, Nilton and Braz, Fabricio and Campos, Teofilo and Guedes, Andre and Mendes, Danilo and Bezerra, Davi and Gusmao, Davi and Chaves, Felipe and Ziegler, Gabriel and Horinouchi, Lucas and Ferreira, Marcelo and Inazawa, Pedro and Coelho, Victor and Fernandes, Ricardo and Peixoto, Fabiano and {Maia Filho}, Mamede and Sukiennik, Bernardo and Rosa, Lahis and Silva, Roberta and Junquilho, Taina and Carvalho, Gustavo},
booktitle = {Proceedings of The Tenth International Conference on Forensic Computer Science and Cyber Law},
doi = {10.5769/C2018001},
isbn = {9788565069151},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Related Work},
month = {10},
pages = {7--11},
publisher = {HTCIA},
title = {{Document type classification for Brazil's supreme court using a Convolutional Neural Network}},
year = {2018}
}


@inproceedings{Radim2010,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks},
      pages = {45--50},
      year = 2010,
      month = 5,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      language={English}
}

@book{Salunke2014,
author = {Salunke, Sagar Shivaji},
title = {Selenium Webdriver in Python: Learn with Examples},
year = {2014},
isbn = {1497337364},
publisher = {CreateSpace Independent Publishing Platform},
address = {North Charleston, SC, USA},
edition = {1st},
abstract = {These days lot of web applications are being developed to meet the growing demands of business. So testing these applications is a big challenge. Automating test scenarios has become almost inevitable to reduce the overall cost and fast regression testing. Selenium webdriver is the best open source testing framework that can be used to automate the testing activities in web application project. In this book I have included all webdriver concepts with examples in Python.}
}



@Misc{Brazil1990,
    author =        {Brazil},
    howpublished = {\url{http://www.planalto.gov.br/ccivil_03/leis/l8078compilado.htm}},
    title        = {Lei Nº 8.078, de 11 de setembro de 1990.},
    year         = {1990},
    note         = {{A}ccessed 10 jan 2020}
}

@book{Goncalves2011,
  title={Responsabilidade civil},
  author={Gon{\c{c}}alves, Carlos Roberto},
  year={2020},
  publisher={Saraiva Educa{\c{c}}{\~a}o SA}
}


@article{Benjamim2015,
author = {Benjamim, Antonio Herman V.},
number = {24},
pages = {23--37},
title = {{O transporte a{\'{e}}reo e o c{\'{o}}digo de defesa do consumidor}},
volume = {100},
year = {2015},
month = {7},
journal = {Revista de Direito do Consumidor}
}

@Misc{Brazil2015,
    author =        {Brazil},
    howpublished = {\url{http://www.planalto.gov.br/ccivil_03/_ato2015-2018/2015/lei/l13105.htm}},
    title        = {Lei No 12.105, de 16 de março de 2015},
    year         = {2015},
    note         = {{A}ccessed 12 jan 2020}
}

@article{Hawkins2004,
author = {Hawkins, Douglas M.},
doi = {10.1021/ci0342472},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hawkins - 2004 - The Problem of Overfitting.pdf:pdf},
issn = {0095-2338},
journal = {Journal of Chemical Information and Computer Sciences},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background,Computer Science/Artificial Intelligence/Machine Learning/Overfitting / Underfitting},
month = {jan},
number = {1},
pages = {1--12},
title = {{The Problem of Overfitting}},
volume = {44},
year = {2004}
}


@Misc{Quintanilla2015,
    author =        {Luis Quintanilla and Nick Schonning  and Nat Kershaw and Youssef Victor and Maira Wenzel and Steven Pratschner and Mykyta Potapenko and C.J. Gronlund and John Alexander and Petr Kulikov},
    howpublished = {\url{https://docs.microsoft.com/en-us/dotnet/machine-learning/resources/tasks}},
    title        = {Machine learning tasks in ML.NET},
    year         = {2015},
    note         = {{A}ccessed 10 jun 2021}
}

@InProceedings{Osinski2004,
author="Osi{\'{n}}ski, Stanis{\l}aw
and Stefanowski, Jerzy
and Weiss, Dawid",
editor="K{\l}opotek, Mieczys{\l}aw A.
and Wierzcho{\'{n}}, S{\l}awomir T.
and Trojanowski, Krzysztof",
title="Lingo: Search Results Clustering Algorithm Based on Singular Value Decomposition",
booktitle="Intelligent Information Processing and Web Mining",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="359--368",
abstract="Search results clustering problem is defined as an automatic, on-line grouping of similar documents in a search results list returned from a search engine. In this paper we present Lingo---a novel algorithm for clustering search results, which emphasizes cluster description quality. We describe methods used in the algorithm: algebraic transformations of the term-document matrix and frequent phrase extraction using suffix arrays. Finally, we discuss results acquired from an empirical evaluation of the algorithm.",
isbn="978-3-540-39985-8"
}


@inproceedings{bird:2004,
address = {Barcelona, Spain},
author = {Bird, Steven and Loper, Edward},
booktitle = {Proceedings of the {ACL} Interactive Poster and Demonstration Sessions},
pages = {214--217},
publisher = {Association for Computational Linguistics},
title = {{NLTK: The Natural Language Toolkit}},
url = {https://www.aclweb.org/anthology/P04-3031},
year = {2004}
}

@book{Cover2005,
author = {Cover, Thomas M. and Thomas, Joy A.},
booktitle = {Elements of Information Theory},
doi = {10.1002/047174882X},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cover, Thomas - 2005 - Elements of Information Theory.pdf:pdf},
isbn = {9780471241959},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
month = {9},
pages = {1--748},
publisher = {Wiley},
title = {{Elements of Information Theory}},
year = {2005}
}


% Diniz, 2020
@article{Diniz2020,
  doi = {10.18316/redes.v8i2.6885},
  year = {2020},
  month = 7,
  publisher = {Centro Universitario La Salle - {UNILASALLE}},
  volume = {8},
  number = {2},
  pages = {181},
  author = {Maria Helena Diniz},
  title = {Prote{\c{c}}{\~{a}}o jur{\'{\i}}dica da existencialidade},
  journal = {Revista Eletr{\^{o}}nica Direito e Sociedade - {REDES}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{Aggarwal2018,
  doi = {10.1007/978-3-319-73531-3},
  year = {2018},
  address = {Cham},
isbn = {9783319735306},
  publisher = {Springer International Publishing},
  author = {Charu C. Aggarwal},
  title = {Machine Learning for Text}
}

@book{Garcia2015,
  doi = {10.1007/978-3-319-10247-4},
  year = {2015},
  publisher = {Springer International Publishing},
  author = {Salvador Garc{\'{\i}}a and Juli{\'{a}}n Luengo and Francisco Herrera},
  title = {Data Preprocessing in Data Mining}
}


@article{Ngo-Ye2014,
author = {Ngo-Ye, Thomas L. and Sinha, Atish P.},
doi = {10.1016/j.dss.2014.01.011},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Online review,RFM analysis,Reviewer engagement characteristics,Text regression,Vector space model},
month = {may},
number = {1},
pages = {47--58},
publisher = {Elsevier B.V.},
title = {{The influence of reviewer engagement characteristics on online review helpfulness: A text regression model}},
volume = {61},
year = {2014}
}

@incollection{Lee1999,
  doi = {10.1007/3-540-48309-8_70},
  year = {1999},
  publisher = {Springer Berlin Heidelberg},
  pages = {751--760},
  author = {Mong Li Lee and Hongjun Lu and Tok Wang Ling and Yee Teng Ko},
  title = {Cleansing Data for Mining and Warehousing},
  booktitle = {Lecture Notes in Computer Science}
}

@Book{Jurafsky2019,
    author = {Jurafsky, D. and Martin, J. H.},
    title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
    publisher = {Draft},
    year = {2019},
    edition = {3rd},
    address = {Stanford University},
}


@Book{Kotu2019,
    author = {Kotu, Vijay and Deshpande, Bala },
    title = {Data Science: Concepts and Practicen},
    year = {2019},
    edition = {2nd},
    address = {Cambridge, MA}, 
    doi = {10.1016/c2017-0-02113-4},
    publisher={Elsevier}
}


@article{Samuel1959,
author = {Samuel, A L},
doi = {10.1147/rd.33.0210},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/1.Definitions},
month = {jul},
number = {3},
pages = {210--229},
title = {{Some Studies in Machine Learning Using the Game of Checkers}},
volume = {3},
year = {1959}
}


@article{Tan2021,
abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
archivePrefix = {arXiv},
arxivId = {2104.00298},
author = {Tan, Mingxing and Le, Quoc V.},
eprint = {2104.00298},
file = {:home/trdp/Downloads/2104.00298.pdf:pdf},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/1.Definitions},
month = {apr},
pages = {1--12},
title = {{EfficientNetV2: Smaller Models and Faster Training}},
volume = {1},
year = {2021}
}


@article{Sengupta2020,
abstract = {Deep learning (DL) has solved a problem that a few years ago was thought to be intractable — the automatic recognition of patterns in spatial and temporal data with an accuracy superior to that of humans. It has solved problems beyond the realm of traditional, hand-crafted machine learning algorithms and captured the imagination of practitioners who are inundated with all types of data. As public awareness of the efficacy of DL increases so does the desire to make use of it. But even for highly trained professionals it can be daunting to approach the rapidly increasing body of knowledge in the field. Where does one start? How does one determine if a particular DL model is applicable to their problem? How does one train and deploy them? With these questions in mind, we present an overview of some of the key DL architectures. We also discuss some new automatic architecture optimization protocols that use multi-agent approaches. Further, since guaranteeing system uptime is critical to many applications, a section dwells on using DL for fault detection and mitigation. This is followed by an exploratory survey of several areas where DL emerged as a game-changer: fraud detection in financial applications, financial time-series forecasting, predictive and prescriptive analytics, medical image processing, power systems research and recommender systems. The thrust of this review is to outline emerging applications of DL and provide a reference to researchers seeking to use DL in their work for pattern recognition with unparalleled learning capacity and the ability to scale with data.},
archivePrefix = {arXiv},
arxivId = {1905.13294},
author = {Sengupta, Saptarshi and Basak, Sanchita and Saikia, Pallabi and Paul, Sayak and Tsalavoutis, Vasilios and Atiah, Frederick and Ravi, Vadlamani and Peters, Alan},
doi = {10.1016/j.knosys.2020.105596},
eprint = {1905.13294},
file = {:home/trdp/Downloads/1-s2.0-S095070512030071X-main.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Applications of deep learning,Deep neural network architectures,Evolutionary computation,Supervised learning,Testing neural networks,Unsupervised learning},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/1.Definitions},
month = {apr},
pages = {105596},
publisher = {Elsevier B.V.},
title = {{A review of deep learning with special emphasis on architectures, applications and recent trends}},
volume = {194},
year = {2020}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
eprint = {1312.6114},
file = {:home/trdp/Downloads/1312.6114.pdf:pdf},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/1.Definitions},
month = {dec},
number = {Ml},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes}},
year = {2013}
}


@article{Silver2017,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
eprint = {1712.01815},
file = {:home/trdp/Downloads/1712.01815.pdf:pdf},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/1.Definitions},
month = {dec},
pages = {1--19},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
year = {2017}
}


@inproceedings{Kumar2015,
  year = {2015},
  publisher = {{ACM} Press},
  author = {Gunupudi Rajesh Kumar and N. Mangathayaru and G. Narasimha},
  title = {Intrusion Detection Using Text Processing Techniques},
  booktitle = {Proceedings of the The International Conference on Engineering {\&} {MIS} 2015 - {ICEMIS} {\textquotesingle}15}
}

@article{Kowsari2019,
author = {Kowsari and {Jafari Meimandi} and Heidarysafa and Mendu and Barnes and Brown},
doi = {10.3390/info10040150},
issn = {2078-2489},
journal = {Information},
keywords = {Document classification,Text analysis,Text categorization,Text classification,Text mining,Text representation},
month = {apr},
number = {4},
pages = {150},
title = {{Text Classification Algorithms: A Survey}},
volume = {10},
year = {2019}
}


@article{Hirschberg2015,
author = {Hirschberg, Julia and Manning, C. D.},
doi = {10.1126/science.aaa8685},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hirschberg, Manning - 2015 - Advances in natural language processing.pdf:pdf},
isbn = {9781405164535},
issn = {0036-8075},
journal = {Science},
keywords = {Language comprehension,Language production},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning},
month = {jul},
number = {6245},
pages = {261--266},
title = {{Advances in natural language processing}},
volume = {349},
year = {2015}
}



@article{Gambhir2017,
author = {Gambhir, Mahak and Gupta, Vishal},
doi = {10.1007/s10462-016-9475-9},
file = {:home/trdp/Downloads/Gambhir-Gupta2017_Article_RecentAutomaticTextSummarizati.pdf:pdf},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Artificial intelligence,Information retrieval,Natural language processing,Summarization survey,Text mining,Text summarization},
mendeley-groups = {Msc Thesis/Refs},
month = {jan},
number = {1},
pages = {1--66},
publisher = {Springer Netherlands},
title = {{Recent automatic text summarization techniques: a survey}},
volume = {47},
year = {2017}
}


@article{Zhou2011,
author = {Zhou, Qiming and Chen, Yumin},
doi = {10.1016/j.isprsjprs.2010.08.005},
file = {:home/trdp/Downloads/1-s2.0-S0924271610000687-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {DEM/DTM,Generalization,Geomorphology,Surface,Triangulation},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background},
month = {jan},
number = {1},
pages = {38--45},
publisher = {Elsevier B.V.},
title = {{Generalization of DEM for terrain analysis using a compound method}},
volume = {66},
year = {2011}
}
@article{Torres2005,
author = {Torres, J.L. and Garc{\'{i}}a, A. and {De Blas}, M. and {De Francisco}, A.},
doi = {10.1016/j.solener.2004.09.013},
file = {:home/trdp/Downloads/1-s2.0-S0038092X04002877-main.pdf:pdf},
issn = {0038092X},
journal = {Solar Energy},
keywords = {Forecasting,Time series,Wind speed},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background},
month = {jul},
number = {1},
pages = {65--77},
title = {{Forecast of hourly average wind speed with ARMA models in Navarre (Spain)}},
volume = {79},
year = {2005}
}



@article{Alzubaidi2021,
author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamar{\'{i}}a, J. and Fadhel, Mohammed A. and Al-Amidie, Muthana and Farhan, Laith},
doi = {10.1186/s40537-021-00444-8},
file = {:home/trdp/Downloads/s40537-021-00444-8.pdf:pdf},
isbn = {4053702100444},
issn = {2196-1115},
journal = {Journal of Big Data},
keywords = {Convolution neural network (CNN),Deep learning,Deep learning applications,Deep neural network architectures,FPGA,GPU,Image classification,Machine learning,Medical image analysis,Supervised learning,Transfer learning},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Pipeline},
month = {dec},
number = {1},
pages = {53},
publisher = {Springer International Publishing},
title = {{Review of deep learning: concepts, CNN architectures, challenges, applications, future directions}},
volume = {8},
year = {2021}
}

@inproceedings{Kornilova2021,
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2101.10737},
author = {Kornilova, Anastasiia and Bernardi, Lucas},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
doi = {10.1145/3437963.3441812},
eprint = {2101.10737},
file = {:home/trdp/Downloads/3437963.3441812.pdf:pdf},
isbn = {9781450382977},
keywords = {2021,acm reference format,anastasiia kornilova and lucas,bernardi,explainability,learning,mining the stars,recommender systems,semi-supervised learning},
month = {mar},
pages = {976--983},
publisher = {ACM},
title = {{Mining the Stars: Learning Quality Ratings with User-facing Explanations for Vacation Rentals}},
year = {2021}
}

@article{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
author = {Hastie, Trevor},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
journal = {The Mathematical Intelligencer},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
number = {2},
pages = {83--85},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
volume = {27},
year = {2009}
}


@article{Hoerl1970,
abstract = {This paper is an exposition of the use of ridge regression methods. Two examples from the literature are used as a base. Attention is focused on the RIDGE TRACE which is a two-dimensional graphical procedure for portraying the complex relationships in multifactor data. Recommendations are made for obtaining a better regression equation than that given by ordinary least squares estimation. {\textcopyright} 1970 Taylor and Francis Group, LLC.},
author = {Hoerl, Arthur E. and Kennard, Robert W.},
doi = {10.1080/00401706.1970.10488635},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoerl, Kennard - 1970 - Ridge Regression Applications to Nonorthogonal Problems.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Models},
number = {1},
pages = {69--82},
title = {{Ridge Regression: Applications to Nonorthogonal Problems}},
volume = {12},
year = {1970}
}


@article{Zou2005,
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00527.x},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zou, Hastie - 2005 - Addendum Regularization and variable selection via the elastic net.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {grouping effect,lars algorithm,lasso,p,penalization},
month = {nov},
number = {5},
pages = {768--768},
title = {{Regularization and variable selection via the elastic net}},
volume = {67},
year = {2005}
}


@article{Tibshirani1996,
author = {Tibshirani, Robert},
doi = {10.1111/j.2517-6161.1996.tb02080.x},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
month = {jan},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection Via the Lasso}},
volume = {58},
year = {1996}
}


@article{Breiman2001,
  doi = {10.1023/a:1010933404324},
  year = {2001},
  title = {Random Forests},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {45},
  number = {1},
  pages = {5--32},
  author = {Leo Breiman},
  journal = {Machine Learning}
}

@article{Friedman2001,
  doi = {10.1214/aos/1013203451},
  year = {2001},
  month = oct,
  publisher = {Institute of Mathematical Statistics},
  volume = {29},
  number = {5},
  author = {Jerome H. Friedman},
  title = {Greedy function approximation: A gradient boosting machine.},
  journal = {The Annals of Statistics}
}

@article{Breiman1996,
author = {Breiman, Leo},
doi = {10.1007/bf00058655},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 1996 - Bagging predictors(2).pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Aggregation,Averaging,Bootstrap,Combining},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Models},
number = {2},
pages = {123--140},
title = {{Bagging predictors}},
volume = {24},
year = {1996}
}


@inproceedings{Schapire1999,
    author = {Schapire, Robert E.},
    title = {A Brief Introduction to Boosting},
    year = {1999},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA},
    booktitle = {Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2},
    pages = {1401–1406},
    numpages = {6},
    location = {Stockholm, Sweden},
    series = {IJCAI'99}
}

@article{Cardoso2018,
author = {Cardoso, Emerson F. and Silva, Renato M. and Almeida, Tiago A.},
doi = {10.1016/j.neucom.2018.04.074},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Fake review,Machine learning,Natural language processing,Spam review,Text categorization},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning/Text Mining & NLP/Classification,Msc Thesis/Refs},
month = {oct},
pages = {106--116},
title = {{Towards automatic filtering of fake reviews}},
volume = {309},
year = {2018}
}



@article{Drucker1997,
author = {Drucker, Harris and Surges, Chris J.C. and Kaufman, Linda and Smola, Alex and Vapnik, Vladimir},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drucker et al. - 1997 - Support vector regression machines.pdf:pdf},
isbn = {0262100657},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Models},
pages = {155--161},
title = {{Support vector regression machines}},
volume = {1},
year = {1997}
}



@article{Mendes-Moreira2012,
author = {Mendes-Moreira, Jo{\~{a}}o and Soares, Carlos and Jorge, Al{\'{i}}pio M{\'{a}}rio and Sousa, Jorge Freire De},
doi = {10.1145/2379776.2379786},
file = {:home/trdp/Downloads/80.pdf:pdf},
isbn = {3512250815},
issn = {0360-0300},
journal = {ACM Computing Surveys},
keywords = {ensembles,regression,supervised learning},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Models},
month = {nov},
number = {1},
pages = {1--40},
title = {{Ensemble approaches for regression}},
volume = {45},
year = {2012}
}

@book{Breiman2017,
author = {Breiman, Leo and Friedman, Jerome H and Olshen, Richard A and Stone, Charles J},
doi = {10.1201/9781315139470},
file = {:home/trdp/Downloads/(The Wadsworth statistics {\_} probability series) Leo Breiman, Jerome Friedman, Richard A. Olshen, Charles J. Stone - Classification and regression trees-CRC (1984).pdf:pdf},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Models},
publisher = {Routledge},
title = {{Classification And Regression Trees}},
year = {2017}
}

@inproceedings{Chen2016,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this
paper, we describe a scalable end-to-end tree boosting system called XGBoost, which
is used widely by data scientists to achieve state-of-the-art results on many machine
learning challenges. We propose a novel sparsity-aware algorithm for sparse data and
weighted quantile sketch for approximate tree learning. More importantly, we provide
insights on cache access patterns, data compression and sharding to build a scalable
tree boosting system. By combining these insights, XGBoost scales beyond billions
of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}




@article{Kingma2015,
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:media/trdp/Arquivos/Studies/Msc/Thesis/Regression{\_}Experiments/Paper Writing/1412.6980.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Models},
pages = {1--15},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}



@inproceedings{Peng2018,
author = {Peng, Hao and Li, Jianxin and He, Yu and Liu, Yaopeng and Bao, Mengjiao and Wang, Lihong and Song, Yangqiu and Yang, Qiang},
title = {Large-Scale Hierarchical Text Classification with Recursively Regularized Deep Graph-CNN},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1063–1072},
numpages = {10},
keywords = {deep learning, hierarchical text classification, convolutional neural networks, graph-of-words, recursive regularization},
location = {Lyon, France},
series = {WWW ’18}
}

@article{Lai2016,
  doi = {10.1109/mis.2016.45},
  year = {2016},
  month = nov,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {31},
  number = {6},
  pages = {5--14},
  author = {Siwei Lai and Kang Liu and Shizhu He and Jun Zhao},
  title = {How to Generate a Good Word Embedding},
  journal = {{IEEE} Intelligent Systems}
}



@article{Alami2019195,
  doi = {10.1016/j.eswa.2019.01.037},
  year = {2019},
  month = 6,
  publisher = {Elsevier {BV}},
  volume = {123},
  pages = {195--211},
  author = {Nabil Alami and Mohammed Meknassi and Noureddine En-nahnahi},
  title = {Enhancing unsupervised neural networks based text summarization with word embedding and ensemble learning},
  journal = {Expert Systems with Applications}
}


@article{Aubaid2018,
author = {Aubaid, Asmaa M. and Mishra, Alok},
issn = {22178333},
journal = {TEM Journal},
keywords = {Rule-based,Systematic mapping,Text classification,Word embedding},
month = {11},
number = {4},
pages = {902--914},
title = {{Text classification using word embedding in Rule-based methodologies: A systematic mapping}},
volume = {7},
doi={10.18421/TEM74-31},
year = {2018}
}

@article{Uysal2016,
  doi = {10.1016/j.eswa.2015.08.050},
  year = {2016},
  month = 1,
  publisher = {Elsevier {BV}},
  volume = {43},
  pages = {82--92},
  author = {Alper Kursat Uysal},
  title = {An improved global feature selection scheme for text classification},
  journal = {Expert Systems with Applications}
}

@article{Ramamoorthy1977,
address = {New York, NY, USA},
author = {Ramamoorthy, C V and Li, H F},
doi = {10.1145/356683.356687},
file = {:home/trdp/Downloads/356683.356687.pdf:pdf},
issn = {0360-0300},
journal = {ACM Computing Surveys},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/1.Definitions},
month = {3},
number = {1},
pages = {61--102},
publisher = {Association for Computing Machinery},
title = {{Pipeline Architecture}},
volume = {9},
year = {1977}
}

@Misc{OBrien2021,
    author =        {Larry O\'Brien and Sheri Gilley  and David Coulter and Peter Lu  and Kraig Brockschmidt  and  Avatar Olivier Martin},
    howpublished = {\url{https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines}},
    title        = {What are Machine Learning Pipelines},
    year         = {2021},
    note         = {{A}ccessed 10 jun 2021}
}


@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/2.Representation},
number = {null},
pages = {993--1022},
publisher = {JMLR.org},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}


@book{Witten2011,
author = {Witten, Ian H and Frank, Eibe and Hall, Mark A},
doi = {10.1016/C2009-0-19715-5},
isbn = {9780123748560},
pages = {664},
pmid = {123748569},
publisher = {Elsevier},
title = {{Data Mining: Practical Machine Learning Tools and Techniques}},
year = {2011}
}



@Book{Jurafsky2019,
    author = {Jurafsky, D. and Martin, J. H.},
    title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
    publisher = {Draft},
    year = {2019},
    edition = {3rd},
    address = {Stanford University},
}

@Book{Garcia2015,
    author = {Salvador Garc{\'{\i}}a and Juli{\'{a}}n Luengo and Francisco Herrera},
    title = {Data Preprocessing in Data Mining},
    publisher = {Springer International Publishing},
    year = {2015},
    url = {https://doi.org/10.1007/978-3-319-10247-4},
}

@incollection{Lee1999,
  url = {https://doi.org/10.1007/3-540-48309-8_70},
  year = {1999},
  publisher = {Springer Berlin Heidelberg},
  pages = {751--760},
  author = {Mong Li Lee and Hongjun Lu and Tok Wang Ling and Yee Teng Ko},
  title = {Cleansing Data for Mining and Warehousing},
  booktitle = {Lecture Notes in Computer Science}
}


@article{Jivani2011,
    author = {Jivani, Anjali},
    year = {2011},
    month = {11},
    pages = {1930--1938},
    title = {A Comparative Study of Stemming Algorithms},
    volume = {2},
    journal = {Int. J. Comp. Tech. Appl.}
}


@article{Chandrashekar2014,
  doi = {10.1016/j.compeleceng.2013.11.024},
  year = {2014},
  month = jan,
  publisher = {Elsevier {BV}},
  volume = {40},
  number = {1},
  pages = {16--28},
  author = {Girish Chandrashekar and Ferat Sahin},
  title = {A survey on feature selection methods},
  journal = {Computers {\&} Electrical Engineering}
}

@article{Guyon2003,
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
pages = {1157--1182},
publisher = {JMLR.org},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}


@article{Jolliffe2016,
abstract = {Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not a priori , hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application.},
author = {Jolliffe, Ian T and Cadima, Jorge},
doi = {10.1098/rsta.2015.0202},
issn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/2.Representation},
month = {apr},
number = {2065},
pages = {20150202},
title = {{Principal component analysis: a review and recent developments}},
volume = {374},
year = {2016}
}


@article{Bojanowski2016,
author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
doi = {10.1162/tacl_a_00051},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning/Text Mining & NLP/Embeddings,Msc Thesis/Embeddings Project,Msc Thesis/Thesis Writing Works/Background/2.Representation},
month = {dec},
pages = {135--146},
title = {{Enriching Word Vectors with Subword Information}},
volume = {5},
year = {2017}
}


@inproceedings{Mohammed2020,
  doi = {10.1109/icoase51841.2020.9436540},
  year = {2020},
  month = dec,
  publisher = {{IEEE}},
  author = {Shapol M. Mohammed and Karwan Jacksi and Subhi R. M. Zeebaree},
  title = {Glove Word Embedding and {DBSCAN} algorithms for Semantic Document Clustering},
  booktitle = {2020 International Conference on Advanced Science and Engineering ({ICOASE})}
}


@article{Meng2018,
author = {Meng, Qinxue and Catchpoole, Daniel and Skillicorn, David and Kennedy, Paul J.},
doi = {10.1109/IJCNN.2017.7965877},
isbn = {9781509061815},
journal = {Proceedings of the International Joint Conference on Neural Networks},
month = {2},
pages = {364--371},
title = {{Relational Autoencoder for Feature Extraction}},
volume = {2017-May},
year = {2018}
}


@article{Vaswani2017,
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
number = {Nips},
pages = {5999--6009},
title = {{Attention Is All You Need}},
volume = {2017-Decem},
year = {2017}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/3.Classification},
month = {nov},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}

@book{Kuhn2013,
  doi = {10.1007/978-1-4614-6849-3},
  year = {2013},
  publisher = {Springer New York},
  author = {Max Kuhn and Kjell Johnson},
  title = {Applied Predictive Modeling}
}


@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:home/trdp/Downloads/1-s2.0-S0893608014002135-main.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background/3.Classification},
pages = {85--117},
pmid = {25462637},
publisher = {Elsevier Ltd},
title = {{Deep Learning in neural networks: An overview}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
volume = {61},
year = {2015}
}


@article{Chai2014,
author = {Chai, T. and Draxler, R. R.},
doi = {10.5194/gmd-7-1247-2014},
file = {:home/trdp/Downloads/gmd-7-1247-2014.pdf:pdf},
issn = {1991-9603},
journal = {Geoscientific Model Development},
month = {jun},
number = {3},
pages = {1247--1250},
title = {{Root mean square error (RMSE) or mean absolute error (MAE)? – Arguments against avoiding RMSE in the literature}},
volume = {7},
year = {2014}
}


@book{Devore2011,
    annote = {ISBN-13: 978-0-538-73352-6},
    author = {Devore, Jay L},
    edition = {8th},
    file = {:home/trdp/Downloads/Jay L. Devore - Probability and Statistics for Engineering and the Sciences, 8th Edition -Cengage Learning (2011).pdf:pdf},
    keywords = {statistics},
    mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
    publisher = {Brooks/Cole},
    title = {{Probability and Statistics for Engineering and the Sciences}},
    year = {2011}
}


@article{Karystinos2000,
abstract = {An algorithmic procedure is developed for the random expansion of a given training set to combat overfitting and improve the generalization ability of backpropagation trained multilayer perceptrons (MLPs). The training set is K-means clustered and locally most entropic colored Gaussian joint input-output probability density function (pdf) estimates are formed per cluster. The number of clusters is chosen such that the resulting overall colored Gaussian mixture exhibits minimum differential entropy upon global cross-validated shaping. Numerical studies on real data and synthetic data examples drawn from the literature illustrate and support these theoretical developments.},
author = {Karystinos, G.N. and Pados, D.A.},
doi = {10.1109/72.870038},
file = {:home/trdp/Downloads/00870038.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
number = {5},
pages = {1050--1057},
title = {{On overfitting, generalization, and randomly expanded training sets}},
volume = {11},
year = {2000}
}



@article{Liu2016,
abstract = {Overfitting has been widely studied in the context of classification and regression. In this paper, we study the overfitting in the context of dimensionality reduction. We show that the conventional wisdom of improving classification performance by maximising inter-class discrimination is not valid for high-dimensional datasets, and can lead to severe overfitting. In particular, we prove the theoretical existence of perfectly discriminative subspace projections, and show that for datasets with very high input dimensionality, inter-class discrimination should be reduced rather than maximised. This naturally leads to a simple dimensionality reduction technique, which we call Soft Discriminant Maps, which we use to show a direct relationship between the classification performance and the level of inter-class discrimination of feature extractors. Moreover, Soft Discriminant Maps consistently exhibit better classification performance than other comparable techniques.},
author = {Liu, Raymond and Gillies, Duncan F.},
doi = {10.1016/j.patcog.2015.11.015},
file = {:home/trdp/Downloads/1-s2.0-S0031320315004355-main.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Classification,Dimensionality reduction,Feature extraction,High-dimensional datasets,Overfitting},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
month = {may},
pages = {73--86},
title = {{Overfitting in linear feature extraction for classification of high-dimensional image data}},
volume = {53},
year = {2016}
}

@book{Weiss2010,
abstract = {Na.},
address = {London},
author = {Weiss, Sholom M. and Indurkhya, Nitin and Zhang, Tong},
booktitle = {Media},
doi = {10.1007/978-1-84996-226-1},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/M.Weiss, Indurkhya, Zhang - 2010 - Fundamentals of Predictive Text Mining.pdf:pdf},
isbn = {978-1-84996-225-4},
issn = {02569574},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning/Text Mining & NLP},
pages = {823},
pmid = {20529422},
publisher = {Springer London},
series = {Texts in Computer Science},
title = {{Fundamentals of Predictive Text Mining}},
volume = {42},
year = {2010}
}


@article{Miao2016,
abstract = {Feature selection, as a dimensionality reduction technique, aims to choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Feature selection usually can lead to better learning performance, i.e., higher learning accuracy, lower computational cost, and better model interpretability. Recently, researchers from computer vision, text mining and so on have proposed a variety of feature selection algorithms and in terms of theory and experiment, show the effectiveness of their works. This paper is aimed at reviewing the state of the art on these techniques. Furthermore, a thorough experiment is conducted to check if the use of feature selection can improve the performance of learning, considering some of the approaches mentioned in the literature. The experimental results show that unsupervised feature selection algorithms benefits machine learning tasks improving the performance of clustering.},
archivePrefix = {arXiv},
arxivId = {1510.02892},
author = {Miao, Jianyu and Niu, Lingfeng},
doi = {10.1016/j.procs.2016.07.111},
eprint = {1510.02892},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miao, Niu - 2016 - A Survey on Feature Selection.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {clustering,feature selection,machine learning,unsupervised},
mendeley-groups = {Computer Science/Artificial Intelligence/Machine Learning/Text Mining {\&} NLP/Feature Extraction},
number = {Itqm},
pages = {919--926},
publisher = {Elsevier Masson SAS},
title = {{A Survey on Feature Selection}},
url = {http://dx.doi.org/10.1016/j.procs.2016.07.111 https://linkinghub.elsevier.com/retrieve/pii/S1877050916313047},
volume = {91},
year = {2016}
}


@article{Freeman1995,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Aims to bring together in a logical framework the vast amount of work on outliers which has been scattered over the years in the various professional journals and texts, and which appears to have acquired a new lease of life over the last decade or so. It is directed to more than one kind of reader: to the student (to inform him of the range of ideas and techniques), to the experimentalist (to assist him in the judicious choice of methods for handling outliers) and to the professional statistician (as a guide to the present state of knowledge and a springbroad for further research. (Wiley Series in Probability and Statistics -Applied Section) 0471 99599 1 378 pages July 1978 {\$}36.00/fl7.50},
author = {Freeman, Jim and Barnett, Vic and Lewis, Toby},
doi = {10.2307/3009915},
file = {:home/trdp/Downloads/(Wiley series in probability and mathematical statistics) Vic Barnett, Toby Lewis - Outliers in Statistical Data-John Wiley and Sons (1978).pdf:pdf},
issn = {01605682},
journal = {The Journal of the Operational Research Society},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
number = {8},
pages = {1034},
title = {{Outliers in Statistical Data.}},
volume = {46},
year = {1995}
}



@article{Hodge2004,
abstract = {This chapter highlights Ningxia Province, with a largely agriculture-based economy that is undergoing both planned (policy-driven) and autonomous (individual-and household-level) transitions. Environmental degradation, overexploitation of water resources and agricultural intensification make the province's ecosystems particularly vulnerable to harm during droughts and other weather hazards, in turn increasing the vulnerability of the populations that depend upon the land and its ecosystem services to support their livelihoods. Ningxia is one of the most arid regions of China, experiencing serious water resource shortages due to lack of supply availability, inefficient use of water supplies and high demand, with high evapotranspiration rates (Xu et al., 2013). The province's average annual per capita water availability is 687 m3, less than one-third of the national average at an amount categorised as ‘severe water shortage' according to World Bank standards (Qiu et al., 2012; see also Chapter 4). Fragile soils, land use changes and multi-year droughts have intensified desertification rates, with degraded areas now accounting for around 44{\%} of provincial land area (Qiu et al., 2012). The types of challenges that Ningxia faces exemplify some aspects of China's larger adaptation challenge. Families in Ningxia that rely on agriculture and livestock raising for their living, like Ma Changjun's, cannot afford to waste a drop of water. His family and those living in Yanchi in Ningxia make their living by growing maize and breeding sheep, and through temporary migration to find work in other places. Many families in rural communities in central and northern parts of the province (Figure 8.1) rely on cisterns to meet drinking water needs, and they ration and recycle water from bathing to wash clothes and water their vegetables and sheep. Despite this, there is not enough water in some years, Ma says in a 2012 interview. Drought, declines in rainfall over the past two decades and overgrazing that damages the land are stressing these families' abilities to survive. The types of issues facing Ningxia's families – land use and water resource management, transitioning socio-economic conditions, pollution issues and migration – are emblematic of the types of issues facing families in other provinces in China, particularly rural families. Many rural provinces, including Ningxia, have greater proportions of ethnic groups who have different cultures, traditional livelihoods and relationships with the land – in effect marking multiple Chinas. These identities and socio-cultural values are shifting in response to wider environmental, policy and economic pressures, as discussed in Chapter 2. Climate change is an additional pressure placed upon the inhabitants of Ningxia, and altering their climate risks.},
author = {Hodge, Victoria and Austin, Jim},
doi = {10.1023/B:AIRE.0000045502.10941.a9},
file = {:media/trdp/Arquivos/Studies/Msc/Thesis/Regression{\_}Experiments/Paper Writing/Hodge-Austin2004{\_}Article{\_}ASurveyOfOutlierDetectionMetho.pdf:pdf},
isbn = {9781317593768},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {anomaly,detection,deviation,noise,novelty,outlier,recognition},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
month = {oct},
number = {2},
pages = {85--126},
title = {{A Survey of Outlier Detection Methodologies}},
volume = {22},
year = {2004}
}



@book{BaezaYates1999,
  added-at = {2009-04-16T13:18:33.000+0200},
  author = {Baeza-Yates, Ricardo A. and Ribeiro-Neto, Berthier A.},
  biburl = {https://www.bibsonomy.org/bibtex/216ab70975f635f8d72de82e2ef3ef9de/hotho},
  description = {dblp},
  interhash = {6f78177742b3c836218aacfc7fc4c43c},
  intrahash = {16ab70975f635f8d72de82e2ef3ef9de},
  isbn = {0-201-39829-X},
  keywords = {information ir lecture retrieval standard vorlesung},
  publisher = {ACM Press / Addison-Wesley},
  timestamp = {2009-04-16T13:18:33.000+0200},
  title = {Modern Information Retrieval},
  url = {http://www.ischool.berkeley.edu/~hearst/irbook/glossary.html},
  year = 1999
}

@article{Liu2008,
abstract = {Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and Random Forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies. {\textcopyright} 2008 IEEE.},
author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi Hua},
doi = {10.1109/ICDM.2008.17},
file = {:home/trdp/Downloads/liu2008.pdf:pdf},
isbn = {9780769535029},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
pages = {413--422},
title = {{Isolation forest}},
year = {2008}
}

@book{Kuhn2013,
author = {Kuhn, Max and Johnson, Kjell},
file = {:home/trdp/Downloads/Max Kuhn, Kjell Johnson - Applied Predictive Modeling-Springer (2013).pdf:pdf},
isbn = {9781461468486},
mendeley-groups = {Msc Thesis/Regression Project/Papers used in Manuscript/Background},
pages = {615},
title = {{Applied Predictive Modeling}},
url = {http://appliedpredictivemodeling.com/s/Applied_Predictive_Modeling_in_R.pdf},
year = {2013}
}


@article{Katz2017,
archivePrefix = {arXiv},
arxivId = {1612.03473},
author = {Katz, Daniel Martin and Bommarito, Michael J. and Blackman, Josh},
doi = {10.1371/journal.pone.0174698},
editor = {Amaral, Lu{\'{i}}s A. Nunes},
eprint = {1612.03473},
file = {:home/trdp/Downloads/pone.0174698.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
month = {4},
number = {4},
pages = {e0174698},
pmid = {28403140},
title = {{A general approach for predicting the behavior of the Supreme Court of the United States}},
volume = {12},
year = {2017}
}


@article{Katz2014,

archivePrefix = {arXiv},
arxivId = {1407.6333},
author = {Katz, Daniel Martin and Bommarito, Michael James and Blackman, Josh},
doi = {10.2139/ssrn.2463244},
eprint = {1407.6333},
file = {:home/trdp/Downloads/1407.6333.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {law,machine learning,quantitative legal prediction,social science,supreme court},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Related Work/classification},
pages = {1--17},
title = {{Predicting the Behavior of the Supreme Court of the United States: A General Approach}},
year = {2014}
}


@article{Sulea2017,
abstract = {In this paper, we investigate the application of text classification methods to predict the law area and the decision of cases judged by the French Supreme Court. We also investigate the influence of the time period in which a ruling was made over the textual form of the case description and the extent to which it is necessary to mask the judge's motivation for a ruling to emulate a real-world test scenario. We report results of 96% f1 score in predicting a case ruling, 90% f1 score in predicting the law area of a case, and 75.9% f1 score in estimating the time span when a ruling has been issued using a linear Support Vector Machine (SVM) classifier trained on lexical features.},
archivePrefix = {arXiv},
arxivId = {1708.01681},
author = {Şulea, Octavia Maria and Zampieri, Marcos and Vela, Mihaela and {Van Genabith}, Josef},
doi = {10.26615/978-954-452-049-6-092},
eprint = {1708.01681},
file = {:home/trdp/Downloads/RANLP092.pdf:pdf},
isbn = {9789544520489},
issn = {13138502},
journal = {International Conference Recent Advances in Natural Language Processing, RANLP},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Related Work/classification},
number = {2011},
pages = {716--722},
title = {{Predicting the law area and decisions of French supreme court cases}},
volume = {2017-Septe},
year = {2017}
}


@inproceedings{Hammami2019,
abstract = {In current years, deep learning has showed promising results when used in the field of natural language processing (NLP). Neural Networks (NNs) such as convolutional neural network (CNN) and recurrent neural network (RNN) have been utilized for different NLP tasks like information retrieval, sentiment analysis and document classification. In this paper, we explore the use of NNs-based method for legal text classification. In our case, the results show that NN models with a fixed input length outperforms baseline methods.},
author = {Hammami, Eya and Akermi, Imen and Faiz, Rim and Boughanem, Mohand},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-32065-2_7},
file = {:home/trdp/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hammami et al. - 2019 - Deep Learning for French Legal Data Categorization.pdf:pdf},
isbn = {9783030320645},
issn = {16113349},
keywords = {Convolutional Neural Networks,Deep learning,Document categorization,Legal domain,Natural Language Processing},
mendeley-groups = {Msc Thesis/RSL - Text Mining & Law/Selected Papers},
pages = {96--105},
title = {{Deep Learning for French Legal Data Categorization}},
volume = {11815 LNCS},
year = {2019}
}

@inproceedings{Maat2010,
author = {de Maat, Emile and Krabben, Kai and Winkels, Radboud},
title = {Machine Learning versus Knowledge Based Classification of Legal Texts},
year = {2010},
isbn = {9781607506812},
publisher = {IOS Press},
address = {NLD},
abstract = {This paper presents results of an experiment in which we used machine learning (ML) techniques to classify sentences in Dutch legislation. These results are compared to the results of a pattern-based classifier. Overall, the ML classifier performs as accurate (&gt;90\%) as the pattern based one, but seems to generalize worse to new laws. Given these results, the pattern based approach is to be preferred since its reasons for classification are clear and can be used for further modelling of the content of the sentences.},
booktitle = {Proceedings of the 2010 Conference on Legal Knowledge and Information Systems: JURIX 2010: The Twenty-Third Annual Conference},
pages = {87–96},
numpages = {10}
}

@article{Acharya2020,
author = {Acharya, Harshith R. and Bhat, Aditya D. and Avinash, K. and Srinath, Ramamoorthy},
doi = {10.3233/JIFS-179870},
editor = {Pinto, David and Singh, Vivek and Perez, Fernando},
file = {:home/trdp/Downloads/10.3233@JIFS-179870.pdf:pdf},
issn = {10641246},
journal = {Journal of Intelligent & Fuzzy Systems},
keywords = {Capsule Network,Law Domain,Natural Language Processing,Sentence Embedding,Text Classification,Unsupervised Extractive Summarization},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Related Work/classification},
month = {aug},
number = {2},
pages = {2037--2046},
title = {{LegoNet - classification and extractive summarization of Indian legal judgments with Capsule Networks and Sentence Embeddings}},
volume = {39},
year = {2020}
}

@article{Quinlan1986,
  doi = {10.1023/a:1022643204877},
  year = {1986},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {1},
  number = {1},
  title = {Induction of Decision Trees},
  pages = {81--106},
  author = {J.R. Quinlan},
  journal = {Machine Learning}
}


@article{Pearson1925,
  doi = {10.2307/2332088},
  year = {1925},
  month = dec,
  publisher = {{JSTOR}},
  volume = {17},
  number = {3/4},
  pages = {388},
  author = {Egon S. Pearson},
  title = {Bayes{\textquotesingle} Theorem,  Examined in the Light of Experimental Sampling},
  journal = {Biometrika}
}

@article{Morgan1988,
  doi = {10.2307/352104},
  year = {1988},
  month = nov,
  publisher = {{JSTOR}},
  volume = {50},
  number = {4},
  pages = {929},
  author = {S. Philip Morgan and Jay D. Teachman},
  title = {Logistic Regression: Description,  Examples,  and Comparisons},
  journal = {Journal of Marriage and the Family}
}

@article{Cortes1995,
  doi = {10.1007/bf00994018},
  year = {1995},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {20},
  number = {3},
  pages = {273--297},
  author = {Corinna Cortes and Vladimir Vapnik},
  title = {Support-vector networks},
  journal = {Machine Learning}
}

@article{Schmidhuber2015,
title = {Deep learning in neural networks: An overview},
journal = {Neural Networks},
volume = {61},
pages = {85-117},
year = {2015},
issn = {0893-6080},
doi = {10.1016/j.neunet.2014.09.003},
author = {Jürgen Schmidhuber},
keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}

@article{Lever2016,
author = {Lever, Jake and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3945},
issn = {1548-7091},
journal = {Nature Methods},
month = {aug},
number = {8},
pages = {603--604},
title = {{Classification evaluation}},
volume = {13},
year = {2016}
}


@article{Kingma2019,
author = {Kingma, Diederik P. and Welling, Max},
doi = {10.1561/2200000056},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
mendeley-groups = {Msc Thesis/Thesis Writing Works/Background},
number = {4},
pages = {307--392},
title = {{An Introduction to Variational Autoencoders}},
volume = {12},
year = {2019}
}

@article{Iqbal2020,
author = {Iqbal, Touseef and Qureshi, Shaima},
doi = {10.1016/j.jksuci.2020.04.001},
issn = {13191578},
journal = {Journal of King Saud University - Computer and Information Sciences},
month = {apr},
number = {xxxx},
pages = {1--14},
publisher = {The Authors},
title = {{The survey: Text generation models in deep learning}},
year = {2020}
}


@BOOK {Hartmann2019,
    author    = "Hartmann, Fabiano  and Silva, Roberta Zumblick Martins da",
    title     = "Inteligência Artificial e Direito",
    publisher = "Alteridade Editora",
    year      = "2019",
    volume    = "1",
    series    = "Direito Racionalidade e Inteligência Artificial",
    edition   = "first"
}

@article{Maaten2008,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579-2605}
}


